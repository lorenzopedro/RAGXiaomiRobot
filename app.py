import os
import sys
from dotenv import load_dotenv

# Carrega as chaves
load_dotenv()

# --- IMPORTA√á√ïES SEGURAS ---
# (Removemos 'langchain.chains' que estava dando erro)
try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_groq import ChatGroq
except ImportError as e:
    print(f"Erro de Instala√ß√£o: {e}")
    sys.exit(1)

# Configura√ß√µes
ARQUIVO_MANUAL = "manual.pdf"

def main():
    print("\nü§ñ --- AGENTE RAG (MODO MANUAL) ---")
    
    # 1. Valida√ß√£o
    if not os.getenv("GROQ_API_KEY"):
        print("‚ùå ERRO: Chave GROQ_API_KEY n√£o configurada no .env")
        return
    
    if not os.path.exists(ARQUIVO_MANUAL):
        print(f"‚ùå ERRO: Arquivo '{ARQUIVO_MANUAL}' n√£o encontrado.")
        return

    # 2. Carregar PDF
    print("1. Lendo PDF...")
    loader = PyPDFLoader(ARQUIVO_MANUAL)
    docs = loader.load()

    # 3. Dividir Texto
    print("2. Processando texto...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    # 4. Criar Banco de Dados (Embeddings)
    print("3. Indexando mem√≥ria (pode demorar 1 min)...")
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory="./chroma_db" # Salva no disco
    )

    # 5. Configurar LLM (Groq)
    print("4. Conectando ao Llama 3...")
    llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0)

    print("\n‚úÖ SISTEMA PRONTO! Pergunte algo.")
    print("-----------------------------------")

    while True:
        pergunta = input("\nVoc√™: ")
        if pergunta.lower() in ["sair", "fim", "exit"]:
            break
        
        print("üîç Pesquisando no manual...")
        
        # --- A M√ÅGICA DO RAG MANUAL (Aqui substitu√≠mos o RetrievalQA) ---
        
        # Passo A: Recupera√ß√£o (Retrieval)
        # Buscamos os 3 trechos mais parecidos com a pergunta
        docs_retornados = vectorstore.similarity_search(pergunta, k=3)
        
        # Juntamos o texto desses trechos para formar o contexto
        contexto = "\n\n".join([doc.page_content for doc in docs_retornados])
        
        # Passo B: Montagem do Prompt
        prompt_final = (
            f"Voc√™ √© um assistente t√©cnico √∫til. Use APENAS o contexto abaixo para responder √† pergunta do usu√°rio.\n"
            f"Se a resposta n√£o estiver no contexto, diga que n√£o sabe.\n\n"
            f"--- CONTEXTO DO MANUAL ---\n{contexto}\n"
            f"--------------------------\n\n"
            f"PERGUNTA DO USU√ÅRIO: {pergunta}"
        )
        
        # Passo C: Gera√ß√£o (Generation)
        try:
            resposta = llm.invoke(prompt_final)
            print(f"\nü§ñ Agente: {resposta.content}")
            
            # Mostra a fonte (Crit√©rio da atividade)
            fonte = docs_retornados[0].metadata.get('page', '?')
            print(f"[Info retirada da p√°gina {fonte}]")
            
        except Exception as e:
            print(f"Erro ao gerar resposta: {e}")

if __name__ == "__main__":
    main()